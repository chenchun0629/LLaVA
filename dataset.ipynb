{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ca2226-7326-4cf8-a521-b5097afad63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llava_instruct_150k.json  llava_instruct_80k.json\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3155a8a6-9ca3-4ca2-be98-93ada4019353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"000000033471\",\n",
      "    \"image\": \"000000033471.jpg\",\n",
      "    \"conversations\": [\n",
      "      {\n",
      "        \"from\": \"human\",\n",
      "        \"value\": \"<image>\\n图中巴士的颜色是 什么?\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"gpt\",\n",
      "        \"value\": \"图中的公共汽车是白色和红色。\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"human\",\n",
      "        \"value\": \"公共汽车后部可看到什么特征?\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"gpt\",\n",
      "        \"value\": \"公共汽车的后部设有广告。\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"human\",\n",
      "        \"value\": \"公共汽车在街上行走 吗?还是停靠在一边 呢?\"\n",
      "      },\n",
      "      {\n",
      "        \"from\": \"gpt\",\n",
      "        \"value\": \"公共汽车正在沿着拥挤着人和其他车辆的街道行驶。\"\n",
      "      }\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "!head -n 30 /workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/llava_instruct_150k.json\n",
    "\n",
    "# llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/llava_instruct_150k.json  \n",
    "# llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/llava_instruct_80k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b022724-1f8b-4713-b439-0d1ac8aa4423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco  gqa  llava_v1_5_mix665k.json  ocr_vqa  text_vqa  vg\n"
     ]
    }
   ],
   "source": [
    "!ls  /workspace/llm_datasets/Visual-Instruction-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d19462a-f229-45bc-9427-16a1bd8d5ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pickleshare\n",
      "Successfully installed pickleshare-0.7.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pickleshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2156ab53-803e-4162-bf81-ec768d05aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'llm_datasets'\n",
      "/workspace/llm_datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd llm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac483c9-d867-4b0d-8d71-e3719bd72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge chinese to llava-pretrain\n",
    "# merge \n",
    "# /workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-CC3M-Pretrain-595K/chat-translated.json\n",
    "# to\n",
    "# /workspace/llm_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5a4920-77d7-4248-9fa3-8042c2dacd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "!cp /workspace/llm_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json /workspace/llm_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json.backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ed4c3e-f2b9-458b-aefb-11f90877e920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595375/595375 [00:00<00:00, 1554394.03it/s]\n",
      "100%|██████████| 558128/558128 [00:00<00:00, 2453243.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "sources = [\n",
    "    {\n",
    "        \"json\": Path('/workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-CC3M-Pretrain-595K/chat-translated.json'),\n",
    "        \"image\": Path('/workspace/llm_datasets/LLaVA-Pretrain'),\n",
    "        \"flag\": \"cc3m\",\n",
    "    },\n",
    "    {\n",
    "        \"json\": Path('/workspace/llm_datasets/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'),\n",
    "        \"image\": Path('/workspace/llm_datasets/LLaVA-Pretrain'),\n",
    "        \"flag\": \"laion\",\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "target = Path('/workspace/llm_datasets/LLaVA-Pretrain/pretrain_data.json')\n",
    "\n",
    "output = []\n",
    "for source in sources:\n",
    "    with source['json'].open('r', encoding='utf-8') as fp:\n",
    "        data = json.load(fp)\n",
    "        for row in tqdm(data):\n",
    "            if source['flag'] == 'cc3m':\n",
    "                row['image'] = f\"images/{row['image']}\"\n",
    "                \n",
    "            # image_file = source['image'] / row['image']\n",
    "            # if not image_file.exists():\n",
    "            #     print(row)\n",
    "            #     continue\n",
    "\n",
    "            output.append(row)\n",
    "\n",
    "\n",
    "with target.open('w', encoding='utf-8') as fp:\n",
    "    json.dump(output, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d17726-db54-4183-a42e-5b29438390ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447M\t/workspace/llm_datasets/LLaVA-Pretrain/pretrain_data.json\n"
     ]
    }
   ],
   "source": [
    "!du -sh /workspace/llm_datasets/LLaVA-Pretrain/pretrain_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5bad222-2066-4155-8eab-2ba5687ade02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157712/157712 [00:04<00:00, 38635.45it/s]\n",
      "100%|██████████| 80000/80000 [00:03<00:00, 24902.65it/s]\n",
      "100%|██████████| 93367/93367 [00:10<00:00, 8834.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15701\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "sources = [\n",
    "    {\n",
    "        \"json\": Path('/workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/llava_instruct_150k.json'),\n",
    "        \"image\": Path('/workspace/llm_datasets/Visual-Instruction-Tuning'),\n",
    "        \"image_prefix\": \"coco/train2017\",\n",
    "        \"flag\": \"llava_150k\",\n",
    "        \"sampling_rate\": 0.02,\n",
    "    },\n",
    "    {\n",
    "        \"json\": Path('/workspace/llm_datasets/Chinese-LLaVA-Vision-Instructions/LLaVA-Instruct-150K/translated/llava_instruct_80k.json'),\n",
    "        \"image\": Path('/workspace/llm_datasets/Visual-Instruction-Tuning'),\n",
    "        \"image_prefix\": \"coco/train2017\",\n",
    "        \"flag\": \"llava_80k\",\n",
    "        \"sampling_rate\": 0.04,\n",
    "    },\n",
    "    {\n",
    "        \"json\": Path('/workspace/llm_datasets/tongue/tongue_train_data5.json'),\n",
    "        \"image\": Path('/workspace/llm_datasets/Visual-Instruction-Tuning'),\n",
    "        \"image_prefix\": \"tongue_image_336\",\n",
    "        \"flag\": \"tongue\",\n",
    "        \"sampling_rate\": 0.1,\n",
    "    },\n",
    "    # {\n",
    "    #     \"json\": Path('/workspace/llm_datasets/Visual-Instruction-Tuning/llava_v1_5_mix665k.json'),\n",
    "    #     \"image\": Path('/workspace/llm_datasets/Visual-Instruction-Tuning'),\n",
    "    #     \"image_prefix\": \"\",\n",
    "    #     \"flag\": \"mix665k\",\n",
    "    # }\n",
    "    \n",
    "]\n",
    "\n",
    "target = Path('/workspace/llm_datasets/Visual-Instruction-Tuning/finetune_data_chinese-5k.json')\n",
    "\n",
    "output = []\n",
    "for source in sources:\n",
    "    with source['json'].open('r', encoding='utf-8') as fp:\n",
    "        data = json.load(fp)\n",
    "        for row in tqdm(data):\n",
    "            if 'image' not in row:\n",
    "                output.append(row)\n",
    "                continue\n",
    "            \n",
    "            if source['flag'] == 'mix665k' and row['image'].startswith('coco/train2017/'):\n",
    "                # 跳过 coco 数据集，使用中文翻译数据集\n",
    "                continue\n",
    "        \n",
    "            if source['image_prefix']:\n",
    "                 row['image'] = (Path(source['image_prefix']) / Path(row['image'])).as_posix()\n",
    "\n",
    "            if 'sampling_rate' in source:\n",
    "                if random.random() <= source['sampling_rate']:\n",
    "                    image_file = source['image'] / row['image']\n",
    "                    if not image_file.exists():\n",
    "                        # print(row)\n",
    "                        continue\n",
    "                        \n",
    "                    output.append(row)\n",
    "            else:\n",
    "                image_file = source['image'] / row['image']\n",
    "                if not image_file.exists():\n",
    "                    # print(row)\n",
    "                    continue\n",
    "                output.append(row)\n",
    "\n",
    "print(len(output))\n",
    "with target.open('w', encoding='utf-8') as fp:\n",
    "    json.dump(output, fp, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f90293a-7859-4529-9233-9b2fee91fb33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Possible classification tags for the listed problems could be:\\n\\n* Security Audit Management:\\n\\t+ Security audit checklist management\\n\\t+ Outdated checklist items and change tracking\\n\\t+ Audit scope definition\\n\\t+ Nessus account management\\n\\t+ Shadow and parallel audits arrangement\\n\\t+ Rescheduling issues\\n* Communication and Collaboration:\\n\\t+ Too many team groups\\n\\t+ Consolidation of pre-release group chats\\n\\t+ Clear communication channels and roles\\n\\t+ Last-minute audit requests\\n\\t+ Making audit scope a required field\\n* Training and Process Improvement:\\n\\t+ Newcomers training process improvement\\n\\nNote that some of the problems could be classified under multiple tags, and the above tags are just one possible way to categorize them.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"WpsM80Y_0\",\n",
      "        \"model\": \"\",\n",
      "        \"conversations\": [\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"这种动物是一种生活在亚洲森林中的哺乳动物。它们通常有一对角和一对鹿茸，身体大小中等。它们的皮毛颜色灰色，并有白色斑点分布在身体上。这些斑点不规则地分布在它们的身上，使它们在森林中很容易被发现。\\n\\n这种动物主要以植物为食，食物包括树叶、嫩枝和草。它们在森林中有着丰富的食物资源，因此它们的生存条件相对较好。它们一般生活在小群体中，由一头雄性和若干头雌性组成。繁殖季节时，雄性会发出特定的叫声来吸引雌性。\\n\\n它是生态系统中重要的一部分，它们对森林生态系统的平衡有着重要的作用。由于它们是许多天敌的猎物，包括老虎、豹、狼等，因此保护这种动物不仅有助于保护它们本身，也能够保护整个生态系统的稳定性。\\n\\n请问文中讲的是什么动物？并说明理由。\\nA、梅花鹿 B、豹子 C、山羊 D、牛\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"根据文中提到的特征，这种动物是梅花鹿。梅花鹿是一种生活在亚洲森林中的哺乳动物，通常有一对角和一对鹿茸，身体大小中等。它们的皮毛颜色灰色，并有白色斑点分布在身体上。这些斑点不规则地分布在它们的身上，使它们在森林中很容易被发现。梅花鹿主要以植物为食，食物包括树叶、嫩枝和草。它们在森林中有着丰富的食物资源，因此它们的生存条件相对较好。它们一般生活在小群体中，由一头雄性和若干头雌性组成。繁殖季节时，雄性会发出特定的叫声来吸引雌性。因此，根据这些特征，可以确定文中讲的动物是梅花鹿。\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"5i0bddh_0\",\n",
      "        \"model\": \"\",\n",
      "        \"conversations\": [\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"I've made a hub site comprised of various employee moments (from onboarding to offboarding and other life events such as getting married, getting sick, etc. in between). The last employee moment is retirement (which should be separate from offboarding). How do you feel the page should look like?\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Congratulations on creating a hub site for your employees' important moments! Including retirement as a separate category from offboarding is a great idea.\\n\\nWhen designing the retirement section of your hub site, here are some suggestions on how you could approach it:\\n\\n1. Create a dedicated page for retirement: Since retirement is a significant milestone in an employee's life, it's best to have a dedicated page that highlights this event. You can create a separate page for retirement, or you can link it to the offboarding page and have a separate section for retirement.\\n2. Feature retired employees: You can showcase retired employees on this page by featuring their pictures, job titles, and years of service. You can also include a short message from the retiree, congratulating them on their retirement and thanking them for their service.\\n3. Provide resources for retirement planning: Since retirement planning can be a daunting task for some employees, consider providing resources to help them prepare for retirement. You can include links to retirement planning tools, financial planning resources, or retirement seminars.\\n4. Highlight retirement benefits: If your company offers retirement benefits, this is an excellent opportunity to promote them on this page. You can include information on retirement plans, pension plans, and other retirement benefits that your company offers.\\n5. Celebrate retirement milestones: You can celebrate retirement milestones such as the number of years an employee has worked at the company or their retirement date. You can also include a section for retired employees' testimonials, where they can share their experiences and offer advice to current employees.\\n6. Keep it positive: Lastly, remember to keep the retirement page positive and celebratory. Retirement is an exciting time for employees, and your page should reflect that. Avoid mentioning negative aspects such as retirement being the end of a career or a person's productive life.\\n\\nBy following these tips, you can create a retirement page that celebrates your employees' achievements and provides helpful resources for those who are planning for retirement.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"XZWuTmr_0\",\n",
      "        \"model\": \"\",\n",
      "        \"conversations\": [\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"I have a set of images, each a scanned attendance sheet containing a list of handwritten student names. Each image is an attendance sheet from a specific day and we have about 20 of those. I want to write some code, using machine learning (or AI) techniques, to decode the names in each image so that I can create a data.frame, one row for each student, and each row having the following columns: the name of the student followed by their attendance status on each of the days (binary variables). I am comfortable with Python and PyTorch (and numpy, etc.)\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"To accomplish this task, you'll need to perform two main steps:\\n\\n1. Handwritten text recognition (HTR) to extract names from the scanned attendance sheets.\\n2. Organize the extracted names and attendance information into a data frame.\\n\\nHere's a step-by-step guide to achieve this using Python and PyTorch:\\n\\n1. Install necessary packages:\\n\\nYou'll need to install pytesseract, pandas, and torchvision for this project:\\n```bash\\npip install pytesseract pandas torchvision\\n```\\n1. Import required libraries:\\n```python\\nimport os\\nimport pytesseract\\nfrom PIL import Image\\nimport pandas as pd\\nimport numpy as np\\nfrom torchvision import transforms\\n```\\n1. Configure pytesseract:\\n\\nMake sure the tesseract binary is in your system's PATH or configure it like this:\\n```python\\n# For Windows\\npytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\\n```\\n1. Create a function to preprocess the image:\\n```python\\ndef preprocess_image(image_path):\\n    image = Image.open(image_path)\\n    transform = transforms.Compose([\\n        transforms.Resize((256, 256)),\\n        transforms.Grayscale(),\\n        transforms.ToTensor(),\\n        transforms.Normalize(mean=[0.5], std=[0.5]),\\n    ])\\n    return transform(image)\\n```\\n1. Create a function to extract names from the attendance sheet:\\n```python\\ndef extract_names(image_path):\\n    image = preprocess_image(image_path)\\n    text = pytesseract.image_to_string(image)\\n    names = [line.strip() for line in text.split('\\\\n') if line.strip()]\\n    return names\\n```\\n1. Process each attendance sheet and store the names and attendance status:\\n```python\\nattendance_sheets = ['path/to/sheet1.jpg', 'path/to/sheet2.jpg', ...] # Add paths to all attendance sheets\\nattendance_dict = {}\\n\\nfor i, sheet_path in enumerate(attendance_sheets):\\n    names = extract_names(sheet_path)\\n    for name in names:\\n        if name not in attendance_dict:\\n            attendance_dict[name] = np.zeros(len(attendance_sheets), dtype=int)\\n        attendance_dict[name][i] = 1\\n```\\n1. Create a pandas data frame with the attendance data:\\n```python\\nattendance_df = pd.DataFrame.from_dict(attendance_dict, orient='index', columns=[f'Day_{i+1}' for i in range(len(attendance_sheets))])\\nattendance_df.insert(0, 'Student_Name', attendance_df.index)\\nattendance_df.reset_index(drop=True, inplace=True)\\n```\\nNow you have the `attendance_df` data frame containing student names and their attendance status for each day. You can further manipulate or save the data frame as needed.\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"I am on a Mac, how do I configure `pytesseract` and acquire the necessary binary?\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"To configure `pytesseract` on a Mac, you need to first install Tesseract OCR. You can install it using Homebrew. If you don't have Homebrew installed, you can install it by following the instructions on their website: <https://brew.sh>\\n\\nOnce you have Homebrew installed, follow these steps:\\n\\n1. Install Tesseract OCR:\\n```bash\\nbrew install tesseract\\n```\\n1. Configure pytesseract:\\n\\nYou don't need to explicitly set the path to the Tesseract binary in your Python script, as Homebrew installs it in a location that is typically included in the system's PATH.\\n\\nHowever, if you run into issues or need to specify the path for some reason, you can find the installation path of the Tesseract binary using the following command:\\n```bash\\nwhich tesseract\\n```\\nThis will return the path to the Tesseract binary. Use this path to set the `tesseract_cmd` in your Python script:\\n```python\\npytesseract.pytesseract.tesseract_cmd = '/path/to/tesseract'\\n```\\nReplace `/path/to/tesseract` with the actual path returned by the `which tesseract` command.\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"The images are stored as PDF. Do I need to process them differently? Some PDF might contain multiple of these attendance sheets as separate pages.\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Yes, you'll need to process PDF files differently since you have to extract images from the PDF pages first. You can use the `pdf2image` library to convert each page in the PDF to an image. To install the library, run:\\n```bash\\npip install pdf2image\\n```\\nYou might also need to install `poppler-utils` to enable the library to work correctly. On macOS, you can install it using Homebrew:\\n```bash\\nbrew install poppler\\n```\\nAfter installing the necessary packages, you can modify the previous code as follows:\\n\\n1. Import the required library:\\n```python\\nfrom pdf2image import convert_from_path\\n```\\n1. Create a function to extract images from a PDF:\\n```python\\ndef pdf_to_images(pdf_path):\\n    return convert_from_path(pdf_path)\\n```\\n1. Process each PDF file and store the names and attendance status:\\n```python\\nattendance_pdfs = ['path/to/pdf1.pdf', 'path/to/pdf2.pdf', ...]  # Add paths to all attendance PDFs\\nattendance_dict = {}\\nday_counter = 0\\n\\nfor pdf_path in attendance_pdfs:\\n    images = pdf_to_images(pdf_path)\\n    for image in images:\\n        text = pytesseract.image_to_string(image)\\n        names = [line.strip() for line in text.split('\\\\n') if line.strip()]\\n        for name in names:\\n            if name not in attendance_dict:\\n                attendance_dict[name] = np.zeros(len(attendance_pdfs) * 20, dtype=int)  # Assuming a maximum of 20 pages per PDF\\n            attendance_dict[name][day_counter] = 1\\n        day_counter += 1\\n\\n# Remove unused columns\\nattendance_dict = {name: np.trim_zeros(status, 'b') for name, status in attendance_dict.items()}\\n```\\n1. Create a pandas data frame with the attendance data:\\n```python\\nattendance_df = pd.DataFrame.from_dict(attendance_dict, orient='index', columns=[f'Day_{i+1}' for i in range(day_counter)])\\nattendance_df.insert(0, 'Student_Name', attendance_df.index)\\nattendance_df.reset_index(drop=True, inplace=True)\\n```\\nNow, the `attendance_df` data frame contains student names and their attendance status for each day, handling PDF files with multiple pages.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"9tCGpLD_0\",\n",
      "        \"model\": \"\",\n",
      "        \"conversations\": [\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"based on the case studies below, generate a website idea to solve problems regarding the case studies.\\n\\nNon-communicable diseases (NCDs), commonly known as chronic diseases,\\nare health conditions that are not transmittable from person to person. Instead,\\nthey result from unhealthy lifestyles and are influenced by a combination of\\ngenetic, physiological, environmental, and behavioral factors. These diseases\\npersist over a long period of time (normally more than 1 year) and can be critical\\nto a person’s health. According to the World Health Organization (WHO), the four\\nmajor types of non-communicable diseases are cardiovascular diseases,\\ncancers, chronic respiratory diseases, and diabetes.\\nSince 2000, the number of deaths caused by non-communicable diseases\\nhas been steadily rising globally. Research indicates that non-communicable\\ndiseases currently account for 74% of all deaths worldwide, claiming an average\\nof 41 million lives each year. The leading cause of NCD-related deaths is\\ncardiovascular diseases, with 17.9 million fatalities annually. Cancers and chronic\\nrespiratory diseases also contribute significantly to NCD-related deaths, with 9.3\\nmillion and 4.1 million deaths respectively, followed by diabetes, including its\\nrelated kidney disease, which accounts for 2 million deaths (WHO, 2022). Non-communicable diseases are a growing concern, affecting individuals of all ages,\\nnot just the late elderly. Each year, 17 million people lost their lives due to non-communicable diseases before reaching 70 years of age, with 85% of these\\npremature (i.e. 30-69 years of age) deaths occurring in low- and middle-income\\ncountries. The four types of non-communicable diseases mentioned earlier are\\nthe main culprits for the majority of all premature NCD fatalities, accounting for\\nover 80% of deaths. (WHO, 2022).\\nThe heavy burden of non-communicable diseases among individuals of\\nworking age results in elevated healthcare expenses, reduced work capacity, and\\nfinancial instability. This unseen epidemic is a frequently overlooked source of\\npoverty and impedes the economic growth of numerous countries. Its impact is\\nintensifying, as the number of individuals, households, and communities affected\\nis on the rise and shows no signs of slowing down. Unfortunately, the occurrence\\nof premature NCD deaths is more prevalent in low- and middle-income countries.\\nThe major cause of this might be due to the rapidly growing population, lacking\\naccess to high-quality healthcare services, practicing unhealthy lifestyles, and\\npoverty. Additionally, 77% of all NCD deaths, regardless of age group, occur in\\nlow- and middle-income countries. This is a concerning issue as NCDs are chronic\\nin nature and demand substantial investment in terms of time and money for\\ntreatment, which can result in financial strain for individuals from low and middle-income countries.\\nrtunately, most non-communicable diseases are preventable. Most non-communicable diseases are caused by common, modifiable risk factors such as\\ntobacco use, excessive alcohol consumption, unhealthy diets, and physical\\ninactivity. By adopting healthy lifestyle habits, the community has the potential to\\nprevent up to 80% of strokes, heart diseases, type 2 diabetes, and over one-third\\nof cancers from taking place. In conclusion, the rise of non-communicable\\ndiseases is not an issue to be taken lightly, as it is the root cause of widespread\\nfatalities, underlying health concerns, and weakened economic development on\\na global scale, particularly in low- and middle-income countries. The impact of\\nthese diseases on global health is a clarion call, urging us to act swiftly and\\ndecisively to prevent and take respective actions to put an end to them. To\\neffectively address this growing health crisis, we must employ effective\\nprevention and management strategies by utilising the existing knowledge we\\nhave today to brainstorm new innovative and cost-effective solutions\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Website Idea: \\\"Healthy Living Hub\\\"\\n\\nThe \\\"Healthy Living Hub\\\" website would be designed to address the growing issue of non-communicable diseases by providing users with access to information, resources, and tools to help them adopt healthy lifestyle habits. The website would aim to be an all-encompassing resource for users to learn about healthy living, including information on healthy diets, physical activity, stress management, and smoking cessation.\\n\\nThe website would feature a variety of tools to help users track their progress towards healthier living. These could include a calorie tracker, exercise tracker, and smoking cessation tracker. Users could set goals and track their progress over time, which would help motivate them to continue on their path towards healthier living.\\n\\nThe website would also include a community section where users could connect with others who are also working towards a healthier lifestyle. This could include forums, groups, and chat rooms where users can share their experiences, ask for advice, and support one another.\\n\\nIn addition to these features, the website could also offer resources such as healthy recipes, workout plans, and stress reduction techniques. It could also provide information on local resources for users, such as gyms, healthy food stores, and smoking cessation programs.\\n\\nOverall, the \\\"Healthy Living Hub\\\" website would provide users with a comprehensive set of tools and resources to help them adopt healthy lifestyle habits and reduce their risk of developing non-communicable diseases. By empowering individuals to take control of their health, we can work towards reducing the burden of these diseases on individuals, communities, and the global economy.\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"give more ideas\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Sure, here are a few more website ideas to address the issue of non-communicable diseases:\\n\\n1. \\\"Healthy Workplace\\\" - This website could be designed to help employers create healthier work environments for their employees. It could include resources and tools to encourage physical activity, healthy eating, stress reduction, and smoking cessation in the workplace. Employers could also access information on workplace wellness programs, employee health screenings, and insurance benefits that support healthy living.\\n2. \\\"Healthy School\\\" - This website could be targeted towards students, parents, and educators to encourage healthy lifestyles in schools. It could provide resources on healthy eating, physical activity, mental health, and tobacco and drug prevention. The website could also feature tools to help schools track their progress towards creating healthier environments, such as a healthy school checklist and a wellness policy template.\\n3. \\\"Healthy Cities\\\" - This website could be designed to promote healthy living in urban environments. It could feature resources on accessible green spaces, bike and walking paths, healthy food options, and smoke-free environments. The website could also include information on city-wide wellness initiatives, such as free fitness classes, health fairs, and community gardens.\\n4. \\\"Healthy Aging\\\" - This website could be targeted towards older adults and their caregivers. It could provide resources on healthy aging, including information on staying active, eating well, managing chronic conditions, and reducing the risk of falls. The website could also feature tools to help older adults track their health and connect with others in their community, such as a senior fitness directory and a caregiver support forum.\\n5. \\\"Healthy Tech\\\" - This website could be designed to promote healthy living in the digital age. It could feature resources on managing screen time, preventing tech-related injuries, and promoting healthy sleep habits. The website could also include information on tech tools that support healthy living, such as fitness tracking apps, mindfulness apps, and sleep tracking devices.\\n\\nOverall, these website ideas could help address the issue of non-communicable diseases by providing individuals, employers, schools, cities, and older adults with the resources they need to adopt healthy lifestyles and reduce their risk of developing chronic conditions.\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"id\": \"Da9D8J4_0\",\n",
      "        \"model\": \"\",\n",
      "        \"conversations\": [\n",
      "            {\n",
      "                \"from\": \"human\",\n",
      "                \"value\": \"Why are anti viral drugs only used for life threatening viruses?\"\n",
      "            },\n",
      "            {\n",
      "                \"from\": \"gpt\",\n",
      "                \"value\": \"Antiviral drugs are not only used for life-threatening viruses. They can be used to treat a wide range of viral infections, including both mild and severe ones. The decision to use antiviral drugs typically depends on the specific virus, the severity of the infection, and the patient's overall health.\\n\\nFor example, antiviral drugs are often used to treat influenza (the flu), which can be a serious and even life-threatening illness for certain people, such as older adults, young children, and people with certain underlying health conditions. Antiviral drugs can help reduce the severity and duration of the flu, and may also help prevent serious complications, such as pneumonia.\\n\\nIn other cases, antiviral drugs may be used to treat viral infections that are not necessarily life-threatening, but can still cause significant discomfort and disrupt a person's daily activities. For example, antiviral drugs may be used to treat herpes simplex virus (HSV) infections, which can cause painful sores on the skin or mucous membranes.\\n\\nOverall, the decision to use antiviral drugs to treat a viral infection depends on a variety of factors, including the specific virus, the severity of the infection, and the potential risks and benefits of treatment.\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "!tail -n 100 /workspace/llm_datasets/Visual-Instruction-Tuning/finetune_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a47ecf1-803d-48e6-865d-556d5d9b9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360M\t/workspace/llm_datasets/Visual-Instruction-Tuning/finetune_data_chinese.json\n"
     ]
    }
   ],
   "source": [
    "!du -sh /workspace/llm_datasets/Visual-Instruction-Tuning/finetune_data_chinese.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851557ec-72b9-4cde-9ab1-509b55c12639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca8375a-04f9-47fc-9946-759615a74b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e896ab2-cea8-47ea-99b3-f35c28abce2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50829f-627f-4efa-bed6-e63b9367b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ea9cc64-f33b-4d8f-879d-529ddde2a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# 创建两个张量\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# 在维度0上堆叠两个张量\n",
    "stacked_tensor = torch.stack([tensor1, tensor2], dim=1)\n",
    "\n",
    "print(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6c087-10d8-4695-834f-7a6e30652db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd9269-8302-4fb7-8bdb-98e4a49603b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7a63c-ad36-428a-a5f0-bc5c940fb50b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
